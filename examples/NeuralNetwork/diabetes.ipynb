{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_diabetes()\n",
    "x, y = data.data, data.target\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_trainscaled = scaler.fit_transform(x_train)\n",
    "x_testscaled = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPRegressor(hidden_layer_sizes=(300, 200 , 200, 100 , 100, 30, 20, 10, 10, 10), activation='relu', max_iter=700,\n",
    "    solver='adam' ,random_state=42, verbose=True, learning_rate='adaptive', learning_rate_init=0.0006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 14884.72747368\n",
      "Iteration 2, loss = 14868.30965482\n",
      "Iteration 3, loss = 14865.48077679\n",
      "Iteration 4, loss = 14866.36855634\n",
      "Iteration 5, loss = 14865.51410361\n",
      "Iteration 6, loss = 14863.61171081\n",
      "Iteration 7, loss = 14861.97458156\n",
      "Iteration 8, loss = 14861.25815424\n",
      "Iteration 9, loss = 14860.35909814\n",
      "Iteration 10, loss = 14859.79473247\n",
      "Iteration 11, loss = 14858.94666500\n",
      "Iteration 12, loss = 14858.12668458\n",
      "Iteration 13, loss = 14857.58387763\n",
      "Iteration 14, loss = 14856.79935263\n",
      "Iteration 15, loss = 14856.22329212\n",
      "Iteration 16, loss = 14855.65019593\n",
      "Iteration 17, loss = 14855.02836395\n",
      "Iteration 18, loss = 14854.47036430\n",
      "Iteration 19, loss = 14853.79905530\n",
      "Iteration 20, loss = 14853.16534009\n",
      "Iteration 21, loss = 14852.50102744\n",
      "Iteration 22, loss = 14851.84750293\n",
      "Iteration 23, loss = 14851.26712871\n",
      "Iteration 24, loss = 14850.60411156\n",
      "Iteration 25, loss = 14850.04333739\n",
      "Iteration 26, loss = 14849.54492003\n",
      "Iteration 27, loss = 14849.00686460\n",
      "Iteration 28, loss = 14848.45821904\n",
      "Iteration 29, loss = 14847.94558250\n",
      "Iteration 30, loss = 14847.38297841\n",
      "Iteration 31, loss = 14846.86423243\n",
      "Iteration 32, loss = 14846.31023707\n",
      "Iteration 33, loss = 14845.74254512\n",
      "Iteration 34, loss = 14845.17992009\n",
      "Iteration 35, loss = 14844.61304827\n",
      "Iteration 36, loss = 14844.02996761\n",
      "Iteration 37, loss = 14843.45429239\n",
      "Iteration 38, loss = 14842.85246041\n",
      "Iteration 39, loss = 14842.24946937\n",
      "Iteration 40, loss = 14841.64185101\n",
      "Iteration 41, loss = 14841.00534299\n",
      "Iteration 42, loss = 14840.36607225\n",
      "Iteration 43, loss = 14839.72425764\n",
      "Iteration 44, loss = 14839.07131154\n",
      "Iteration 45, loss = 14838.40163471\n",
      "Iteration 46, loss = 14837.72578100\n",
      "Iteration 47, loss = 14837.02821950\n",
      "Iteration 48, loss = 14836.32623466\n",
      "Iteration 49, loss = 14835.60920615\n",
      "Iteration 50, loss = 14834.88204083\n",
      "Iteration 51, loss = 14834.13870749\n",
      "Iteration 52, loss = 14833.36891061\n",
      "Iteration 53, loss = 14832.59812018\n",
      "Iteration 54, loss = 14831.79270428\n",
      "Iteration 55, loss = 14830.99155595\n",
      "Iteration 56, loss = 14830.15549314\n",
      "Iteration 57, loss = 14829.28195503\n",
      "Iteration 58, loss = 14828.39225954\n",
      "Iteration 59, loss = 14827.48401476\n",
      "Iteration 60, loss = 14826.57020092\n",
      "Iteration 61, loss = 14825.61923689\n",
      "Iteration 62, loss = 14824.63857241\n",
      "Iteration 63, loss = 14823.63238781\n",
      "Iteration 64, loss = 14822.56675625\n",
      "Iteration 65, loss = 14821.48436469\n",
      "Iteration 66, loss = 14820.36625296\n",
      "Iteration 67, loss = 14819.23068508\n",
      "Iteration 68, loss = 14818.02772389\n",
      "Iteration 69, loss = 14816.85637630\n",
      "Iteration 70, loss = 14815.57731589\n",
      "Iteration 71, loss = 14814.38417467\n",
      "Iteration 72, loss = 14813.14183861\n",
      "Iteration 73, loss = 14811.80068343\n",
      "Iteration 74, loss = 14810.47278883\n",
      "Iteration 75, loss = 14809.14910211\n",
      "Iteration 76, loss = 14807.74252302\n",
      "Iteration 77, loss = 14806.21193755\n",
      "Iteration 78, loss = 14804.62437235\n",
      "Iteration 79, loss = 14803.00444244\n",
      "Iteration 80, loss = 14801.18957788\n",
      "Iteration 81, loss = 14799.40886939\n",
      "Iteration 82, loss = 14797.31933645\n",
      "Iteration 83, loss = 14795.07448793\n",
      "Iteration 84, loss = 14792.59562909\n",
      "Iteration 85, loss = 14789.59423999\n",
      "Iteration 86, loss = 14785.86615830\n",
      "Iteration 87, loss = 14781.49131974\n",
      "Iteration 88, loss = 14776.07327865\n",
      "Iteration 89, loss = 14769.16451864\n",
      "Iteration 90, loss = 14759.34378570\n",
      "Iteration 91, loss = 14745.01210505\n",
      "Iteration 92, loss = 14721.48990203\n",
      "Iteration 93, loss = 14682.53342992\n",
      "Iteration 94, loss = 14622.30249418\n",
      "Iteration 95, loss = 14503.20759754\n",
      "Iteration 96, loss = 14305.41226415\n",
      "Iteration 97, loss = 14002.19914452\n",
      "Iteration 98, loss = 13470.40784321\n",
      "Iteration 99, loss = 12621.64584776\n",
      "Iteration 100, loss = 11348.69852965\n",
      "Iteration 101, loss = 9482.01443874\n",
      "Iteration 102, loss = 6996.51025792\n",
      "Iteration 103, loss = 4135.80720088\n",
      "Iteration 104, loss = 2783.84157997\n",
      "Iteration 105, loss = 3935.64395736\n",
      "Iteration 106, loss = 3307.87124267\n",
      "Iteration 107, loss = 2403.19937528\n",
      "Iteration 108, loss = 2439.52800421\n",
      "Iteration 109, loss = 2749.19238055\n",
      "Iteration 110, loss = 2590.04750627\n",
      "Iteration 111, loss = 2131.28228644\n",
      "Iteration 112, loss = 1851.20535502\n",
      "Iteration 113, loss = 1997.75904878\n",
      "Iteration 114, loss = 2074.47877066\n",
      "Iteration 115, loss = 1862.09741588\n",
      "Iteration 116, loss = 1726.97964561\n",
      "Iteration 117, loss = 1762.56870970\n",
      "Iteration 118, loss = 1801.71455488\n",
      "Iteration 119, loss = 1743.26700322\n",
      "Iteration 120, loss = 1657.39286516\n",
      "Iteration 121, loss = 1624.24426444\n",
      "Iteration 122, loss = 1656.05398712\n",
      "Iteration 123, loss = 1647.89277380\n",
      "Iteration 124, loss = 1596.16392421\n",
      "Iteration 125, loss = 1577.89243389\n",
      "Iteration 126, loss = 1572.67089881\n",
      "Iteration 127, loss = 1569.99481297\n",
      "Iteration 128, loss = 1550.10755873\n",
      "Iteration 129, loss = 1529.45537526\n",
      "Iteration 130, loss = 1521.56574803\n",
      "Iteration 131, loss = 1518.17327166\n",
      "Iteration 132, loss = 1510.33606682\n",
      "Iteration 133, loss = 1498.82783321\n",
      "Iteration 134, loss = 1489.11878947\n",
      "Iteration 135, loss = 1488.57056198\n",
      "Iteration 136, loss = 1480.05528580\n",
      "Iteration 137, loss = 1471.66052468\n",
      "Iteration 138, loss = 1463.63702127\n",
      "Iteration 139, loss = 1459.11319693\n",
      "Iteration 140, loss = 1455.05028939\n",
      "Iteration 141, loss = 1448.73462205\n",
      "Iteration 142, loss = 1441.61242743\n",
      "Iteration 143, loss = 1437.11675149\n",
      "Iteration 144, loss = 1431.19553220\n",
      "Iteration 145, loss = 1426.56485983\n",
      "Iteration 146, loss = 1421.65251456\n",
      "Iteration 147, loss = 1416.86045467\n",
      "Iteration 148, loss = 1411.46764322\n",
      "Iteration 149, loss = 1407.51701134\n",
      "Iteration 150, loss = 1402.82185031\n",
      "Iteration 151, loss = 1398.26021313\n",
      "Iteration 152, loss = 1393.56725020\n",
      "Iteration 153, loss = 1390.93024588\n",
      "Iteration 154, loss = 1385.21288079\n",
      "Iteration 155, loss = 1386.30979099\n",
      "Iteration 156, loss = 1380.21193494\n",
      "Iteration 157, loss = 1373.45455063\n",
      "Iteration 158, loss = 1370.54000010\n",
      "Iteration 159, loss = 1366.01342067\n",
      "Iteration 160, loss = 1362.51761278\n",
      "Iteration 161, loss = 1358.89237730\n",
      "Iteration 162, loss = 1355.55689544\n",
      "Iteration 163, loss = 1351.45464944\n",
      "Iteration 164, loss = 1348.96599677\n",
      "Iteration 165, loss = 1345.15705357\n",
      "Iteration 166, loss = 1342.01748393\n",
      "Iteration 167, loss = 1338.23287981\n",
      "Iteration 168, loss = 1334.83461950\n",
      "Iteration 169, loss = 1332.28540169\n",
      "Iteration 170, loss = 1329.01039941\n",
      "Iteration 171, loss = 1325.23291233\n",
      "Iteration 172, loss = 1322.69874059\n",
      "Iteration 173, loss = 1319.71941667\n",
      "Iteration 174, loss = 1316.79089453\n",
      "Iteration 175, loss = 1313.24988047\n",
      "Iteration 176, loss = 1310.18090810\n",
      "Iteration 177, loss = 1307.70023392\n",
      "Iteration 178, loss = 1304.79172879\n",
      "Iteration 179, loss = 1304.32595545\n",
      "Iteration 180, loss = 1300.17023091\n",
      "Iteration 181, loss = 1297.82371138\n",
      "Iteration 182, loss = 1293.66282025\n",
      "Iteration 183, loss = 1290.57164794\n",
      "Iteration 184, loss = 1287.88202607\n",
      "Iteration 185, loss = 1285.71127181\n",
      "Iteration 186, loss = 1283.12212574\n",
      "Iteration 187, loss = 1279.46499333\n",
      "Iteration 188, loss = 1279.02213618\n",
      "Iteration 189, loss = 1276.60351772\n",
      "Iteration 190, loss = 1272.04277242\n",
      "Iteration 191, loss = 1270.46202125\n",
      "Iteration 192, loss = 1266.71403885\n",
      "Iteration 193, loss = 1263.31224027\n",
      "Iteration 194, loss = 1263.24575794\n",
      "Iteration 195, loss = 1261.38651446\n",
      "Iteration 196, loss = 1256.46946091\n",
      "Iteration 197, loss = 1254.31716817\n",
      "Iteration 198, loss = 1251.34844365\n",
      "Iteration 199, loss = 1250.47232492\n",
      "Iteration 200, loss = 1247.59584789\n",
      "Iteration 201, loss = 1243.13520872\n",
      "Iteration 202, loss = 1242.75644586\n",
      "Iteration 203, loss = 1241.25912665\n",
      "Iteration 204, loss = 1237.89825189\n",
      "Iteration 205, loss = 1234.95098139\n",
      "Iteration 206, loss = 1232.63840501\n",
      "Iteration 207, loss = 1228.96256539\n",
      "Iteration 208, loss = 1226.56782325\n",
      "Iteration 209, loss = 1223.50059006\n",
      "Iteration 210, loss = 1225.55266133\n",
      "Iteration 211, loss = 1221.09064105\n",
      "Iteration 212, loss = 1218.51021795\n",
      "Iteration 213, loss = 1215.80587147\n",
      "Iteration 214, loss = 1213.87938077\n",
      "Iteration 215, loss = 1209.39420085\n",
      "Iteration 216, loss = 1206.79495536\n",
      "Iteration 217, loss = 1204.48694259\n",
      "Iteration 218, loss = 1204.46774396\n",
      "Iteration 219, loss = 1199.97049981\n",
      "Iteration 220, loss = 1198.83086290\n",
      "Iteration 221, loss = 1193.90763528\n",
      "Iteration 222, loss = 1193.76808390\n",
      "Iteration 223, loss = 1188.46738604\n",
      "Iteration 224, loss = 1185.92118104\n",
      "Iteration 225, loss = 1183.77542907\n",
      "Iteration 226, loss = 1180.97484501\n",
      "Iteration 227, loss = 1178.89399396\n",
      "Iteration 228, loss = 1175.38566649\n",
      "Iteration 229, loss = 1174.04426445\n",
      "Iteration 230, loss = 1171.06102010\n",
      "Iteration 231, loss = 1167.30724233\n",
      "Iteration 232, loss = 1166.96425317\n",
      "Iteration 233, loss = 1163.55679490\n",
      "Iteration 234, loss = 1159.15775288\n",
      "Iteration 235, loss = 1157.43855802\n",
      "Iteration 236, loss = 1154.57950242\n",
      "Iteration 237, loss = 1151.03496365\n",
      "Iteration 238, loss = 1148.44372237\n",
      "Iteration 239, loss = 1145.91060234\n",
      "Iteration 240, loss = 1142.56174790\n",
      "Iteration 241, loss = 1139.46671705\n",
      "Iteration 242, loss = 1136.80703267\n",
      "Iteration 243, loss = 1133.84462289\n",
      "Iteration 244, loss = 1130.36430489\n",
      "Iteration 245, loss = 1128.60256513\n",
      "Iteration 246, loss = 1124.75448947\n",
      "Iteration 247, loss = 1121.72048494\n",
      "Iteration 248, loss = 1118.36758282\n",
      "Iteration 249, loss = 1114.53221733\n",
      "Iteration 250, loss = 1112.38487668\n",
      "Iteration 251, loss = 1109.27651302\n",
      "Iteration 252, loss = 1106.51590708\n",
      "Iteration 253, loss = 1101.30036333\n",
      "Iteration 254, loss = 1099.30352028\n",
      "Iteration 255, loss = 1097.86149488\n",
      "Iteration 256, loss = 1094.49002091\n",
      "Iteration 257, loss = 1088.44720141\n",
      "Iteration 258, loss = 1084.40829264\n",
      "Iteration 259, loss = 1082.26494741\n",
      "Iteration 260, loss = 1078.77993705\n",
      "Iteration 261, loss = 1073.72505848\n",
      "Iteration 262, loss = 1069.97751621\n",
      "Iteration 263, loss = 1066.90170957\n",
      "Iteration 264, loss = 1064.13895315\n",
      "Iteration 265, loss = 1061.66095079\n",
      "Iteration 266, loss = 1056.64608563\n",
      "Iteration 267, loss = 1053.30387286\n",
      "Iteration 268, loss = 1048.27028624\n",
      "Iteration 269, loss = 1045.58635491\n",
      "Iteration 270, loss = 1039.79281311\n",
      "Iteration 271, loss = 1037.16227344\n",
      "Iteration 272, loss = 1032.68898334\n",
      "Iteration 273, loss = 1029.32602127\n",
      "Iteration 274, loss = 1023.90620784\n",
      "Iteration 275, loss = 1023.19800238\n",
      "Iteration 276, loss = 1016.84849902\n",
      "Iteration 277, loss = 1012.05054405\n",
      "Iteration 278, loss = 1007.85350770\n",
      "Iteration 279, loss = 1002.52494230\n",
      "Iteration 280, loss = 998.27012497\n",
      "Iteration 281, loss = 995.28063397\n",
      "Iteration 282, loss = 993.01222730\n",
      "Iteration 283, loss = 986.07046526\n",
      "Iteration 284, loss = 979.61484626\n",
      "Iteration 285, loss = 975.64396027\n",
      "Iteration 286, loss = 973.98203029\n",
      "Iteration 287, loss = 965.39468215\n",
      "Iteration 288, loss = 962.22331901\n",
      "Iteration 289, loss = 963.29952053\n",
      "Iteration 290, loss = 950.75266536\n",
      "Iteration 291, loss = 948.52644859\n",
      "Iteration 292, loss = 944.52622751\n",
      "Iteration 293, loss = 936.34100646\n",
      "Iteration 294, loss = 931.47652774\n",
      "Iteration 295, loss = 927.67792328\n",
      "Iteration 296, loss = 920.84600877\n",
      "Iteration 297, loss = 914.99361752\n",
      "Iteration 298, loss = 911.32039848\n",
      "Iteration 299, loss = 905.26836585\n",
      "Iteration 300, loss = 899.62758039\n",
      "Iteration 301, loss = 893.95705714\n",
      "Iteration 302, loss = 887.96114720\n",
      "Iteration 303, loss = 882.14676872\n",
      "Iteration 304, loss = 877.75168148\n",
      "Iteration 305, loss = 870.30140326\n",
      "Iteration 306, loss = 865.21602343\n",
      "Iteration 307, loss = 860.42362747\n",
      "Iteration 308, loss = 852.90900831\n",
      "Iteration 309, loss = 848.21678380\n",
      "Iteration 310, loss = 840.80403267\n",
      "Iteration 311, loss = 834.90793527\n",
      "Iteration 312, loss = 830.00991665\n",
      "Iteration 313, loss = 823.30802371\n",
      "Iteration 314, loss = 816.28922628\n",
      "Iteration 315, loss = 809.67752570\n",
      "Iteration 316, loss = 803.44702510\n",
      "Iteration 317, loss = 798.19079108\n",
      "Iteration 318, loss = 789.51953350\n",
      "Iteration 319, loss = 783.37991735\n",
      "Iteration 320, loss = 777.01473872\n",
      "Iteration 321, loss = 769.80346190\n",
      "Iteration 322, loss = 763.02640334\n",
      "Iteration 323, loss = 755.77028975\n",
      "Iteration 324, loss = 747.97960358\n",
      "Iteration 325, loss = 741.27306632\n",
      "Iteration 326, loss = 734.51252962\n",
      "Iteration 327, loss = 726.51516217\n",
      "Iteration 328, loss = 719.87387849\n",
      "Iteration 329, loss = 713.72184029\n",
      "Iteration 330, loss = 704.51364065\n",
      "Iteration 331, loss = 699.01604416\n",
      "Iteration 332, loss = 688.47463829\n",
      "Iteration 333, loss = 684.01235138\n",
      "Iteration 334, loss = 676.17904621\n",
      "Iteration 335, loss = 667.20094754\n",
      "Iteration 336, loss = 663.09184472\n",
      "Iteration 337, loss = 651.61717713\n",
      "Iteration 338, loss = 646.59947057\n",
      "Iteration 339, loss = 635.71967964\n",
      "Iteration 340, loss = 643.11150599\n",
      "Iteration 341, loss = 619.25060582\n",
      "Iteration 342, loss = 622.73245267\n",
      "Iteration 343, loss = 613.67357274\n",
      "Iteration 344, loss = 603.73526841\n",
      "Iteration 345, loss = 592.52724150\n",
      "Iteration 346, loss = 586.41426316\n",
      "Iteration 347, loss = 575.54889354\n",
      "Iteration 348, loss = 570.48226173\n",
      "Iteration 349, loss = 558.25319894\n",
      "Iteration 350, loss = 558.23080120\n",
      "Iteration 351, loss = 551.42511763\n",
      "Iteration 352, loss = 541.03288479\n",
      "Iteration 353, loss = 530.06412358\n",
      "Iteration 354, loss = 531.52257398\n",
      "Iteration 355, loss = 514.19608757\n",
      "Iteration 356, loss = 505.86868456\n",
      "Iteration 357, loss = 500.75983196\n",
      "Iteration 358, loss = 490.79429626\n",
      "Iteration 359, loss = 487.21368650\n",
      "Iteration 360, loss = 476.51979259\n",
      "Iteration 361, loss = 466.55274455\n",
      "Iteration 362, loss = 460.54189163\n",
      "Iteration 363, loss = 450.43187668\n",
      "Iteration 364, loss = 444.89914739\n",
      "Iteration 365, loss = 433.95095754\n",
      "Iteration 366, loss = 425.59739542\n",
      "Iteration 367, loss = 417.63375008\n",
      "Iteration 368, loss = 408.88230984\n",
      "Iteration 369, loss = 405.45807285\n",
      "Iteration 370, loss = 391.68516144\n",
      "Iteration 371, loss = 388.94694961\n",
      "Iteration 372, loss = 374.69670294\n",
      "Iteration 373, loss = 372.85459002\n",
      "Iteration 374, loss = 361.47722708\n",
      "Iteration 375, loss = 352.37458385\n",
      "Iteration 376, loss = 345.20360042\n",
      "Iteration 377, loss = 336.41866915\n",
      "Iteration 378, loss = 329.37385335\n",
      "Iteration 379, loss = 322.96981600\n",
      "Iteration 380, loss = 315.56569251\n",
      "Iteration 381, loss = 304.03439258\n",
      "Iteration 382, loss = 301.12181686\n",
      "Iteration 383, loss = 290.94267219\n",
      "Iteration 384, loss = 285.54254121\n",
      "Iteration 385, loss = 273.89689139\n",
      "Iteration 386, loss = 268.64082003\n",
      "Iteration 387, loss = 260.43403878\n",
      "Iteration 388, loss = 252.01631711\n",
      "Iteration 389, loss = 246.60210994\n",
      "Iteration 390, loss = 243.12930414\n",
      "Iteration 391, loss = 236.37391550\n",
      "Iteration 392, loss = 224.06450149\n",
      "Iteration 393, loss = 227.65346855\n",
      "Iteration 394, loss = 213.40080186\n",
      "Iteration 395, loss = 209.03031448\n",
      "Iteration 396, loss = 203.72105664\n",
      "Iteration 397, loss = 204.88012796\n",
      "Iteration 398, loss = 196.45956699\n",
      "Iteration 399, loss = 186.94702593\n",
      "Iteration 400, loss = 182.01431328\n",
      "Iteration 401, loss = 186.49589543\n",
      "Iteration 402, loss = 191.54974650\n",
      "Iteration 403, loss = 179.18276879\n",
      "Iteration 404, loss = 180.43957762\n",
      "Iteration 405, loss = 167.62400974\n",
      "Iteration 406, loss = 160.09921207\n",
      "Iteration 407, loss = 158.36684666\n",
      "Iteration 408, loss = 144.73112492\n",
      "Iteration 409, loss = 141.91199942\n",
      "Iteration 410, loss = 139.35100604\n",
      "Iteration 411, loss = 134.38938727\n",
      "Iteration 412, loss = 130.57844829\n",
      "Iteration 413, loss = 123.85367797\n",
      "Iteration 414, loss = 123.16558744\n",
      "Iteration 415, loss = 118.30876497\n",
      "Iteration 416, loss = 118.34827253\n",
      "Iteration 417, loss = 114.80103756\n",
      "Iteration 418, loss = 107.45774891\n",
      "Iteration 419, loss = 107.90767665\n",
      "Iteration 420, loss = 105.68301435\n",
      "Iteration 421, loss = 110.03846327\n",
      "Iteration 422, loss = 102.81670786\n",
      "Iteration 423, loss = 99.90351059\n",
      "Iteration 424, loss = 98.55002854\n",
      "Iteration 425, loss = 93.68795965\n",
      "Iteration 426, loss = 94.37304946\n",
      "Iteration 427, loss = 90.89382631\n",
      "Iteration 428, loss = 83.99225217\n",
      "Iteration 429, loss = 82.96766768\n",
      "Iteration 430, loss = 80.64339091\n",
      "Iteration 431, loss = 79.54399590\n",
      "Iteration 432, loss = 76.21039104\n",
      "Iteration 433, loss = 73.06894715\n",
      "Iteration 434, loss = 72.81968502\n",
      "Iteration 435, loss = 72.88210407\n",
      "Iteration 436, loss = 70.79562064\n",
      "Iteration 437, loss = 71.05389976\n",
      "Iteration 438, loss = 69.29025371\n",
      "Iteration 439, loss = 71.26870580\n",
      "Iteration 440, loss = 60.89878636\n",
      "Iteration 441, loss = 63.10167758\n",
      "Iteration 442, loss = 64.00751189\n",
      "Iteration 443, loss = 59.90334099\n",
      "Iteration 444, loss = 60.79981399\n",
      "Iteration 445, loss = 59.19125207\n",
      "Iteration 446, loss = 56.77052219\n",
      "Iteration 447, loss = 58.27639729\n",
      "Iteration 448, loss = 57.11599059\n",
      "Iteration 449, loss = 54.04060142\n",
      "Iteration 450, loss = 56.59385197\n",
      "Iteration 451, loss = 48.79402768\n",
      "Iteration 452, loss = 49.61006890\n",
      "Iteration 453, loss = 47.18554964\n",
      "Iteration 454, loss = 49.24828218\n",
      "Iteration 455, loss = 49.62589899\n",
      "Iteration 456, loss = 47.74542987\n",
      "Iteration 457, loss = 41.07567970\n",
      "Iteration 458, loss = 42.08500070\n",
      "Iteration 459, loss = 40.68388676\n",
      "Iteration 460, loss = 40.34151894\n",
      "Iteration 461, loss = 37.57275682\n",
      "Iteration 462, loss = 37.91346017\n",
      "Iteration 463, loss = 37.84220702\n",
      "Iteration 464, loss = 33.69024909\n",
      "Iteration 465, loss = 35.00054985\n",
      "Iteration 466, loss = 33.13904057\n",
      "Iteration 467, loss = 32.41687548\n",
      "Iteration 468, loss = 32.07560955\n",
      "Iteration 469, loss = 30.24429225\n",
      "Iteration 470, loss = 30.18390092\n",
      "Iteration 471, loss = 28.63623122\n",
      "Iteration 472, loss = 28.57231082\n",
      "Iteration 473, loss = 28.18219333\n",
      "Iteration 474, loss = 28.03605797\n",
      "Iteration 475, loss = 26.44112966\n",
      "Iteration 476, loss = 26.38117788\n",
      "Iteration 477, loss = 26.61268146\n",
      "Iteration 478, loss = 24.98778889\n",
      "Iteration 479, loss = 25.16700199\n",
      "Iteration 480, loss = 24.18518581\n",
      "Iteration 481, loss = 23.40961607\n",
      "Iteration 482, loss = 23.48938176\n",
      "Iteration 483, loss = 22.24036205\n",
      "Iteration 484, loss = 21.72894430\n",
      "Iteration 485, loss = 21.59671551\n",
      "Iteration 486, loss = 21.17283603\n",
      "Iteration 487, loss = 20.90441763\n",
      "Iteration 488, loss = 20.08716213\n",
      "Iteration 489, loss = 19.59888288\n",
      "Iteration 490, loss = 19.04771879\n",
      "Iteration 491, loss = 18.87425934\n",
      "Iteration 492, loss = 18.50883262\n",
      "Iteration 493, loss = 18.45894206\n",
      "Iteration 494, loss = 18.07421340\n",
      "Iteration 495, loss = 17.49437037\n",
      "Iteration 496, loss = 16.95262259\n",
      "Iteration 497, loss = 16.82947135\n",
      "Iteration 498, loss = 16.21745063\n",
      "Iteration 499, loss = 15.85176855\n",
      "Iteration 500, loss = 15.90309912\n",
      "Iteration 501, loss = 15.19287100\n",
      "Iteration 502, loss = 15.34360468\n",
      "Iteration 503, loss = 15.50588112\n",
      "Iteration 504, loss = 15.55994810\n",
      "Iteration 505, loss = 14.43787452\n",
      "Iteration 506, loss = 13.98361029\n",
      "Iteration 507, loss = 13.52397925\n",
      "Iteration 508, loss = 13.30940169\n",
      "Iteration 509, loss = 13.03091672\n",
      "Iteration 510, loss = 13.37583554\n",
      "Iteration 511, loss = 13.92080697\n",
      "Iteration 512, loss = 13.24227062\n",
      "Iteration 513, loss = 13.40707543\n",
      "Iteration 514, loss = 13.78424424\n",
      "Iteration 515, loss = 13.49491892\n",
      "Iteration 516, loss = 13.35439870\n",
      "Iteration 517, loss = 11.81719546\n",
      "Iteration 518, loss = 11.84534060\n",
      "Iteration 519, loss = 12.60056711\n",
      "Iteration 520, loss = 12.96422286\n",
      "Iteration 521, loss = 12.45586440\n",
      "Iteration 522, loss = 11.50838128\n",
      "Iteration 523, loss = 10.80847290\n",
      "Iteration 524, loss = 11.48728556\n",
      "Iteration 525, loss = 10.57023742\n",
      "Iteration 526, loss = 9.55027990\n",
      "Iteration 527, loss = 9.64076961\n",
      "Iteration 528, loss = 9.28572558\n",
      "Iteration 529, loss = 8.96314456\n",
      "Iteration 530, loss = 8.75594551\n",
      "Iteration 531, loss = 8.44715920\n",
      "Iteration 532, loss = 8.40491312\n",
      "Iteration 533, loss = 8.34457720\n",
      "Iteration 534, loss = 8.27213488\n",
      "Iteration 535, loss = 7.95684090\n",
      "Iteration 536, loss = 8.07089082\n",
      "Iteration 537, loss = 8.10175279\n",
      "Iteration 538, loss = 7.93965181\n",
      "Iteration 539, loss = 7.33142612\n",
      "Iteration 540, loss = 8.02668889\n",
      "Iteration 541, loss = 6.97861921\n",
      "Iteration 542, loss = 6.78639316\n",
      "Iteration 543, loss = 7.18302443\n",
      "Iteration 544, loss = 6.80839714\n",
      "Iteration 545, loss = 6.53424193\n",
      "Iteration 546, loss = 6.31528542\n",
      "Iteration 547, loss = 6.14092999\n",
      "Iteration 548, loss = 6.05975509\n",
      "Iteration 549, loss = 5.82814044\n",
      "Iteration 550, loss = 5.95514227\n",
      "Iteration 551, loss = 6.16596840\n",
      "Iteration 552, loss = 5.53369916\n",
      "Iteration 553, loss = 5.55074841\n",
      "Iteration 554, loss = 5.23655893\n",
      "Iteration 555, loss = 5.23665252\n",
      "Iteration 556, loss = 5.08977783\n",
      "Iteration 557, loss = 5.11195269\n",
      "Iteration 558, loss = 4.85197037\n",
      "Iteration 559, loss = 4.79625220\n",
      "Iteration 560, loss = 4.51373889\n",
      "Iteration 561, loss = 4.59687318\n",
      "Iteration 562, loss = 4.71811230\n",
      "Iteration 563, loss = 4.34532800\n",
      "Iteration 564, loss = 4.25117117\n",
      "Iteration 565, loss = 4.24835056\n",
      "Iteration 566, loss = 4.18285979\n",
      "Iteration 567, loss = 3.99735114\n",
      "Iteration 568, loss = 3.89303352\n",
      "Iteration 569, loss = 3.87622835\n",
      "Iteration 570, loss = 3.72193414\n",
      "Iteration 571, loss = 3.68734489\n",
      "Iteration 572, loss = 3.82980965\n",
      "Iteration 573, loss = 3.62585272\n",
      "Iteration 574, loss = 3.43820678\n",
      "Iteration 575, loss = 3.39515676\n",
      "Iteration 576, loss = 3.25600017\n",
      "Iteration 577, loss = 3.14943065\n",
      "Iteration 578, loss = 3.07256374\n",
      "Iteration 579, loss = 3.00390459\n",
      "Iteration 580, loss = 2.94347651\n",
      "Iteration 581, loss = 3.01600650\n",
      "Iteration 582, loss = 3.16736957\n",
      "Iteration 583, loss = 2.86457603\n",
      "Iteration 584, loss = 2.75875945\n",
      "Iteration 585, loss = 2.91636234\n",
      "Iteration 586, loss = 2.75423812\n",
      "Iteration 587, loss = 2.63517497\n",
      "Iteration 588, loss = 2.66905810\n",
      "Iteration 589, loss = 2.51673332\n",
      "Iteration 590, loss = 2.52858365\n",
      "Iteration 591, loss = 2.64715382\n",
      "Iteration 592, loss = 2.53889419\n",
      "Iteration 593, loss = 2.73281401\n",
      "Iteration 594, loss = 2.57712515\n",
      "Iteration 595, loss = 2.57714262\n",
      "Iteration 596, loss = 2.39749588\n",
      "Iteration 597, loss = 2.30848586\n",
      "Iteration 598, loss = 2.14051421\n",
      "Iteration 599, loss = 2.14588365\n",
      "Iteration 600, loss = 2.17657397\n",
      "Iteration 601, loss = 2.32914808\n",
      "Iteration 602, loss = 2.04694785\n",
      "Iteration 603, loss = 2.09734158\n",
      "Iteration 604, loss = 1.96974077\n",
      "Iteration 605, loss = 1.89860361\n",
      "Iteration 606, loss = 1.99348807\n",
      "Iteration 607, loss = 1.84442264\n",
      "Iteration 608, loss = 1.93781402\n",
      "Iteration 609, loss = 1.82829719\n",
      "Iteration 610, loss = 1.89603069\n",
      "Iteration 611, loss = 1.83954907\n",
      "Iteration 612, loss = 1.83507930\n",
      "Iteration 613, loss = 1.83353608\n",
      "Iteration 614, loss = 2.02163069\n",
      "Iteration 615, loss = 1.71495526\n",
      "Iteration 616, loss = 1.65410419\n",
      "Iteration 617, loss = 1.60993999\n",
      "Iteration 618, loss = 1.69749624\n",
      "Iteration 619, loss = 1.93380609\n",
      "Iteration 620, loss = 1.91967947\n",
      "Iteration 621, loss = 1.77514948\n",
      "Iteration 622, loss = 1.74001230\n",
      "Iteration 623, loss = 1.80339132\n",
      "Iteration 624, loss = 1.67241210\n",
      "Iteration 625, loss = 1.54842701\n",
      "Iteration 626, loss = 1.59228431\n",
      "Iteration 627, loss = 1.59184861\n",
      "Iteration 628, loss = 1.95405454\n",
      "Iteration 629, loss = 1.83597962\n",
      "Iteration 630, loss = 1.76365832\n",
      "Iteration 631, loss = 1.70575461\n",
      "Iteration 632, loss = 1.97075735\n",
      "Iteration 633, loss = 1.71417405\n",
      "Iteration 634, loss = 1.36878010\n",
      "Iteration 635, loss = 1.53365326\n",
      "Iteration 636, loss = 1.63099807\n",
      "Iteration 637, loss = 1.97976707\n",
      "Iteration 638, loss = 1.89469491\n",
      "Iteration 639, loss = 1.41382018\n",
      "Iteration 640, loss = 1.38178270\n",
      "Iteration 641, loss = 1.29651903\n",
      "Iteration 642, loss = 1.46434158\n",
      "Iteration 643, loss = 1.52725290\n",
      "Iteration 644, loss = 1.43400837\n",
      "Iteration 645, loss = 1.34401698\n",
      "Iteration 646, loss = 1.73745160\n",
      "Iteration 647, loss = 1.59552077\n",
      "Iteration 648, loss = 1.32682317\n",
      "Iteration 649, loss = 1.14407224\n",
      "Iteration 650, loss = 1.13566697\n",
      "Iteration 651, loss = 0.99798381\n",
      "Iteration 652, loss = 0.91105354\n",
      "Iteration 653, loss = 1.00794108\n",
      "Iteration 654, loss = 0.96371803\n",
      "Iteration 655, loss = 0.93625575\n",
      "Iteration 656, loss = 0.97075835\n",
      "Iteration 657, loss = 0.96424648\n",
      "Iteration 658, loss = 0.88593823\n",
      "Iteration 659, loss = 1.09997385\n",
      "Iteration 660, loss = 1.27996919\n",
      "Iteration 661, loss = 1.38652221\n",
      "Iteration 662, loss = 1.66414160\n",
      "Iteration 663, loss = 1.82222809\n",
      "Iteration 664, loss = 1.90548801\n",
      "Iteration 665, loss = 1.63774355\n",
      "Iteration 666, loss = 1.47766934\n",
      "Iteration 667, loss = 1.50122116\n",
      "Iteration 668, loss = 1.57388463\n",
      "Iteration 669, loss = 1.60964907\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-29 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-29 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-29 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-29 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-29 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-29 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-29 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-29 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-29 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-29 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-29 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-29 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-29 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-29 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-29 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-29 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-29 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-29 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-29 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-29\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=(300, 200, 200, 100, 100, 30, 20, 10, 10, 10),\n",
       "             learning_rate=&#x27;adaptive&#x27;, learning_rate_init=0.0006, max_iter=700,\n",
       "             random_state=42, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" checked><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPRegressor(hidden_layer_sizes=(300, 200, 200, 100, 100, 30, 20, 10, 10, 10),\n",
       "             learning_rate=&#x27;adaptive&#x27;, learning_rate_init=0.0006, max_iter=700,\n",
       "             random_state=42, verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(300, 200, 200, 100, 100, 30, 20, 10, 10, 10),\n",
       "             learning_rate='adaptive', learning_rate_init=0.0006, max_iter=700,\n",
       "             random_state=42, verbose=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.fit(x_trainscaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17191077187986203"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.predict(x_testscaled)\n",
    "MLP.score(x_testscaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
